\documentclass[a4paper]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{amssymb}
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{bm}

\title{Auto-Encoding Variational Bayes}
\date{}

\begin{document}

\maketitle

\section{Citation}
Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).

\begin{verbatim}
https://arxiv.org/pdf/1312.6114.pdf
\end{verbatim}

\section{Abstract}
By estimating an intractable posterior distribution with an approximate 
inference model (trained with SGD), we can make inferences in directed
graphical models with continuous latent variables, intractable posteriors,
and larger datasets.

\section{Introduction}
Variational Bayes (VB) is used to approximate intractable posteriors, but can 
sometimes itself be intractable. With a clever reparameterization, we propose
a Stochastic Gradient VB (SGVB) estimator that can approximate the posterior.
For an i.i.d dataset with continuous latent variables, we use the Auto-Encoding
VB (AEVB) model, which can be used as a SGVB estimator. When this model is a
neural network, we call it a variational autoencoder.

\section{Method}
Suppose we have a dataset generated by (1) generating latent variables 
$\bm{z^{(i)}}$  from a prior $p_{\theta^*}(\bm{z})$ and (2) the data point
is generated with $p_{\theta^*}(\bm{x} | \bm{z})$. We assume that the posterior
$p(\bm{z} | \bm{x})$ is intractable and the dataset is large (so Markov Chain
Monte Carlo won't work).

We aim to (1) infer $\bm{\theta}$ from the data, (2) infer $\bm{z}$ from
$\bm{\theta}$ and $\bm{x}$, and (3) marginalize over $\bm{x}$. We approximate
the posterior $p_{\theta}(\bm{z} | \bm{x})$ with a recognition
network (also called an encoder) $q_{\phi}(\bm{z} | \bm{x})$. We observe that


\begin{align}
  \log{p_{\theta}(\bm{x}^{(i)})} = D_{KL}(q_{\phi}(\bm{z} | \bm{x}^{(i)})
  || p_{\theta}(\bm{z} | \bm{x^{(i)}})) + \mathcal{L}(\bm{\theta}, \bm{\phi};
  \bm{x}^{(i)})
\end{align}

The second term, the variational lower bound, is $\mathcal{L}(\bm{\theta},
\bm{\phi}; \bm{x}^{(i)}) = -D_{KL}(q_{\theta}(\bm{z} | \bm{x}^{(i)}) ||
p_{\theta}(\bm{z})) + \mathbb{E}_{q_{\phi}(\bm{z} | \bm{x}^{(i)})}[
  \log{p_{\theta}(\bm{x}^{(i)} | \bm{z})}
]$

We further parameterize $\tilde{\bm{z}} = g_{\theta}(\bm{\epsilon}, \bm{x})$
where $\bm{\epsilon} \sim p(\bm{\epsilon})$ (this enables easier sampling).
After some other simplifications, we can estimate the variational lower bound
with:

\begin{align}
  \tilde{\mathcal{L}}^B(\bm{\theta}, \bm{\phi}; \bm{x}^{(i)})
  = -D_{KL}(
    q_{\theta}(\bm{z} | \bm{x}^{(i)})
    || p_{\theta}(\bm{z})
  ) + \frac{1}{L} \sum_{l=1}^{L}{\log(p_{\theta}(
    \bm{x}^{(i)} | \bm{z}^{(i, l)}
  ))}
\end{align}

Given a minibatch of size $M$, we can estimate this as $\mathcal{L}(
\bm{\theta}, \bm{\phi}; \bm{X}) \approx \tilde{\mathcal{L}}^M(\bm{\theta},
\bm{\phi}, \bm{X}^{M}) = \frac{N}{M} \sum_{i=1}^{M}{\tilde{\mathcal{L}}(
  \bm{\theta}, \bm{\phi}, \bm{x}^{(i)}
)}$

We set $M = 100$ and $L = 1$. In the approximation $\tilde{\mathcal{L}}$,
the KL divergence acts as a regularizer pushing the recognition function towards
the prior. The recognition function encodes noise to produce $\bm{z}$, which is
then reconstructed with the likelihood function to create $\bm{x}^{(i)}$.

Notice our reparameterization trick where we have defined $p(\epsilon)$ as
some distribution that we are allowed to choose. We can make this distribution
something that is easy to sample from.

\section{Example: Variational Auto-Encoder}
We'll use a neural network to approximate $q_{\phi}(\bm{z} | \bm{x})$.
We assume $p_\theta(\bm{z}) \sim \mathcal{N}(\bm{z}; \bm{0}, \bm{I})$. We
assume $p_{\theta}(\bm{x} | \bm{z})$s is a multivariate Gaussian (or 
Bernoulli if the data is binary) where the distribution parameters are estimated
from a multi-layer perceptron (MLP) parameterized by $\bm{\theta}$ and operating
on $\bm{z}$. We further assume that

\begin{align}
  \log(q_{\theta}(\bm{z} | \bm{x}^{(i)}))
  \sim \log(\mathcal{N}(\bm{z}; \mu^{(i)}, \bm{\sigma}^{(i)} \bm{I}))
\end{align}

where $\bm{\mu}^{(i)}$ and $\bm{\sigma}^{(i)}$ are computed from an MLP
parameterized by $\bm{\phi}$ and operating on $\bm{x}^{(i)}$. We let
$\bm{\epsilon}^{(l)} \sim \mathcal{N}(0, \bm{I})$ and we thus compute:

\begin{align}
  & \mathcal{L}(\bm{\theta}, \bm{\phi}; \bm{x}^{(i)})
  \approx \frac{1}{2} \sum_{j=1}^{J}{
    (1 + \log((\sigma_j^{(i)})^2) - (\mu_j^{(i)})^2 - (\sigma_j^{(i)})^2 )
    + \frac{1}{L} \sum_{l=1}^{L}{\log(p_{\theta}(
      \bm{x}^{(i)} | \bm{z}^{(i, l)}
    ))}
    } \\
  & \bm{z}^{(i, l)} = \bm{\mu}^{(i)} + \bm{\sigma}^{(i)} \odot
  \bm{\epsilon}^{(l)} \\
  & \bm{\epsilon}^{(l)} \sim \mathcal{N}(0, \bm{I})
\end{align}

\section{Related Work}
The wake-sleep function also approximates the recognition function, but requires
two objective functions. There are techniques to regularize autoencoders.
Generative Stochastic Networks use noisy autoencoders to learn transition
operator of a Markov chain.

\section{Experiments}
We train on MNIST and Frey Faces. MNIST had Bernoulli output units while
Frey Faces had Gaussian (constrained to range [0, 1] with a sigmoid unit)
output units. We train with Adagrad with weight decay. Weights were initialized
from a zero-mean Gaussian. We compare against wake-sleep.

We trained generative models (decoders) and recognition models (encoders) for
the likelihood lower bound. We also did this for the marginal likelihood.

\section{Conclusion}
We show how to efficiently approximate the variational lower bound using
a neural network trained with SGD.

\section{Future Work}
We can learn hierarchical generative architectures, time series models, 
and apply this to the global parameters of a directed graphical model.

\end{document}
